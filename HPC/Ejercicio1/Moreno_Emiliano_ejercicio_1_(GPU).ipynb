{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moreno_Emiliano_ejercicio_1_(GPU).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-Vno_15t-E"
      },
      "source": [
        "# 1 Introducción\n",
        "El siguiente cuaderno realiza la multiplicación de dos vectores, utilizando GPGPU. El resultado se almacena en un tercer vector. El kernel ejecuta en hilos sobre una dimensión (x).\n",
        "\n",
        "M=αA+B\n",
        "\n",
        "Para la resolución se aplicaron conceptos de Lenguaje Python, CUDA y el manejo de operaciones aritméticas sobre vectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRnhv_7N4Pa"
      },
      "source": [
        "---\n",
        "# 2 Armado del ambiente\n",
        "Instala en el cuaderno el módulo CUDA de Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74FNbCszDmw"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzQaWRTtc1Zj"
      },
      "source": [
        "---\n",
        "# 3 Desarrollo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c7mZSnu0M3m"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   5000#@param {type: \"number\"}\n",
        "alfa =   1#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# CPU - Defino la memoria de los vectores en cpu.\n",
        "A_cpu = numpy.random.randn( cantidad_N )\n",
        "A_cpu = A_cpu.astype( numpy.float32() )\n",
        "\n",
        "B_cpu = numpy.random.randn( cantidad_N )\n",
        "B_cpu = B_cpu.astype( numpy.float32() )\n",
        "\n",
        "M_cpu = numpy.random.randn( cantidad_N )\n",
        "M_cpu = M_cpu.astype( numpy.float32() )\n",
        "\n",
        "tiempo_ini_cpu = datetime.now()\n",
        "\n",
        "# CPU - reservo la memoria GPU.\n",
        "A_gpu = cuda.mem_alloc( A_cpu.nbytes )\n",
        "B_gpu = cuda.mem_alloc( B_cpu.nbytes )\n",
        "M_gpu = cuda.mem_alloc( M_cpu.nbytes )\n",
        "\n",
        "# GPU - Copio la memoria al GPU.\n",
        "cuda.memcpy_htod( A_gpu, A_cpu )\n",
        "cuda.memcpy_htod( B_gpu, B_cpu )\n",
        "cuda.memcpy_htod( M_gpu, M_cpu )\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "__global__ void Multplicacion_Vectores( int n, float alfa, float *X, float *Y, float *M)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  if( idx < n )\n",
        "  {\n",
        "    M[idx]  = alfa*X[idx] * Y[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\") \n",
        "\n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"Multplicacion_Vectores\")\n",
        "\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "dim_hilo = 256\n",
        "dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "#Kernel\n",
        "kernel( numpy.int32(cantidad_N),numpy.float32(alfa), A_gpu, B_gpu, M_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "## GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( M_cpu, M_gpu)\n",
        "\n",
        "#\"\"\"\n",
        "# CPU - Informo el resutlado.\n",
        "print( \"------------------------------------\")\n",
        "print( \"Vector A: \" )\n",
        "print( A_cpu )\n",
        "print( \"------------------------------------\")\n",
        "print( \"Vector B: \" )\n",
        "print( B_cpu )\n",
        "print( \"------------------------------------\")\n",
        "print( \"Multiplicación A X B: \" )\n",
        "print( M_cpu )\n",
        "#\"\"\"\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print( \"Cantidad de elementos: \", cantidad_N )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EALIlyyG6iqP"
      },
      "source": [
        "---\n",
        "# 4 Tabla de pasos de ejecución del programa\n",
        "\n",
        "\n",
        " Procesador | Funciòn | Detalle\n",
        "------------|---------|----------\n",
        "CPU      |  @param                | Lectura del tamaño de vectores desde Colab.\n",
        "CPU      |  import                | Importa los módulos para funcionar.\n",
        "CPU      |  datetime.now()        | Toma el tiempo actual.\n",
        "CPU      |  numpy.random.randn( Cantidad_N ) | Inicializa los vectores A, B y M.\n",
        "**GPU**  |  cuda.mem_alloc()      | Reserva la memoria en GPU.\n",
        "**GPU**  |  cuda.memcpy_htod()    | Copia las memorias desde el CPU al GPU.\n",
        "CPU      |  SourceModule()        | Define el código del kernel \n",
        "CPU      |  module.get_function() | Genera la función del kernel GPU\n",
        "CPU      |  dim_tx/dim_bx         | Calcula las dimensiones.\n",
        "**GPU**  |  kernel()              | Ejecuta el kernel en GPU\n",
        "CPU      |  cuda.memcpy_dtoh( )   | Copia el resultado desde GPU memoria M a CPU memoria M.\n",
        "CPU      |  print()               | Informo los resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzgZkrQD-UTy"
      },
      "source": [
        "---\n",
        "# 5 Conclusiones\n",
        "\n",
        "A medida que aumenta la cantidad de elementos (N) del vector aumenta el tiempo de procesamiento. A pesar de que en GPU tenemos más funciones y pasos para ejecutar (ej. reservar la memoria en area de GPU, copia de datos, definir y compilar Kernel, ejecución de Kernel) podemos notar que al realizar la multiplicación de dos vectores de N elementos hacerlo con GPU reduce mucho el tiempo total comparado con el tiempo total de la misma operación solo con CPU. Esto ultimo se debe a que en GPU la ejecución se realiza mediante hilos concurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hn6HOCYEjyY"
      },
      "source": [
        "---\n",
        "# 6 Bibliografia\n",
        "\n",
        "Introducción a Python: [Página Colab](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/Python_Basico.ipynb) \n",
        "\n",
        "Documentación PyCUDA: [WEB](https://documen.tician.de/pycuda/index.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}